{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea74013-5091-4606-8895-c58b77ba9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from shutil import copyfile\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81f8739-d378-413b-ac0b-ac1eadd5d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import admix\n",
    "from os.path import join\n",
    "import itertools\n",
    "import admix_genet_cor\n",
    "import submitit\n",
    "import glob\n",
    "from scipy.stats import pearsonr, linregress\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b567e422-cb39-498e-9764-f51d56f2ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwas_clump(config, sim_i):\n",
    "\n",
    "    region_idx = config.split(\"-\")[1]\n",
    "\n",
    "    pfile = f\"out/datasets/region{region_idx}\"\n",
    "    dset = admix.io.read_dataset(\n",
    "        pfile=pfile,\n",
    "        n_anc=2,\n",
    "    )\n",
    "    df_sample_info = pd.read_csv(\n",
    "        f\"out/pheno/{config}.pheno.tsv.gz\", sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    # extract covariates and perform standardization\n",
    "    df_cov = dset.indiv[[\"AVG_ANC\"]]\n",
    "    df_cov = (df_cov - df_cov.mean(axis=0)) / df_cov.std(axis=0)\n",
    "    df_sample_info = pd.merge(\n",
    "        df_sample_info[[f\"SIM_{sim_i}\"]],\n",
    "        df_cov,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    if not os.path.exists(f\"out/clump/{config}\"):\n",
    "        os.makedirs(f\"out/clump/{config}\")\n",
    "\n",
    "    for chrom in range(1, 2):\n",
    "        out_prefix = f\"out/clump/{config}/sim_{sim_i}.chr{chrom}\"\n",
    "        admix.tools.plink2.gwas(\n",
    "            pfile=pfile,\n",
    "            df_sample_info=df_sample_info,\n",
    "            pheno_col=f\"SIM_{sim_i}\",\n",
    "            covar_cols=[\"AVG_ANC\"],\n",
    "            out_prefix=out_prefix,\n",
    "            clean_tmp_file=True,\n",
    "            chr=chrom,\n",
    "            memory=int(18 * 1e3),\n",
    "        )\n",
    "        # See Pardinas et al. for description of these set of parameters\n",
    "        admix.tools.plink.clump(\n",
    "            bfile=pfile,\n",
    "            assoc_path=out_prefix + \".assoc\",\n",
    "            out_prefix=out_prefix,\n",
    "            p1=5e-8,\n",
    "            p2=1e-4,\n",
    "            r2=0.1,\n",
    "            kb=10000,  # 10Mb clumping window\n",
    "            chr=chrom,\n",
    "            memory=int(18 * 1e3),\n",
    "        )\n",
    "        os.remove(out_prefix + \".assoc\")\n",
    "\n",
    "    # merging all the clumping\n",
    "    df_clump = []\n",
    "    clump_file_prefix = f\"out/clump/{config}/sim_{sim_i}\"\n",
    "    for chrom in range(1, 2):\n",
    "        clumped_file = clump_file_prefix + f\".chr{chrom}.clumped\"\n",
    "        with open(clumped_file) as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 1:\n",
    "                continue\n",
    "        df_clump.append(pd.read_csv(clumped_file, delim_whitespace=True))\n",
    "    df_clump = pd.concat(df_clump).sort_values([\"CHR\", \"BP\"])\n",
    "    df_clump.to_csv(clump_file_prefix + f\".clumped\", index=False, sep=\"\\t\")\n",
    "    # clean up\n",
    "    for f in glob.glob(clump_file_prefix + \".chr*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "def test_het(config, sim_i):\n",
    "\n",
    "    ###########\n",
    "    # read info\n",
    "    ###########\n",
    "    region_idx = config.split(\"-\")[1]\n",
    "\n",
    "    pfile = f\"out/datasets/region{region_idx}\"\n",
    "\n",
    "    df_clump = (\n",
    "        pd.read_csv(f\"out/clump/{config}/sim_{sim_i}.clumped\", sep=\"\\t\")\n",
    "        .set_index(\"SNP\")\n",
    "        .sort_values([\"CHR\", \"BP\"])\n",
    "    )\n",
    "    df_pheno = pd.read_csv(f\"out/pheno/{config}.pheno.tsv.gz\", sep=\"\\t\", index_col=0)[\n",
    "        f\"SIM_{sim_i}\"\n",
    "    ]\n",
    "\n",
    "    df_beta = pd.DataFrame(\n",
    "        index=np.loadtxt(f\"out/pheno/{config}.snplist.gz\", dtype=str)\n",
    "    )\n",
    "    # attached effect sizes\n",
    "    df_beta[[\"AFR_BETA\", \"EUR_BETA\"]] = np.load(f\"out/pheno/{config}.beta.npz\")[\n",
    "        \"arr_0\"\n",
    "    ][:, :, sim_i]\n",
    "    #     assert np.allclose(df_beta[\"AFR_BETA\"], df_beta[\"EUR_BETA\"])\n",
    "\n",
    "    # extract only causal SNPs\n",
    "    dict_snp_list = {\n",
    "        \"causal\": df_beta[\n",
    "            (df_beta.AFR_BETA != 0) | (df_beta.EUR_BETA != 0)\n",
    "        ].index.values,\n",
    "        \"clump\": df_clump.index.values,\n",
    "    }\n",
    "\n",
    "    dict_df_summary = {\"clump\": [], \"causal\": []}\n",
    "\n",
    "    for chrom in range(1, 2):\n",
    "        dset = admix.io.read_dataset(\n",
    "            pfile=pfile,\n",
    "            snp_chunk=128,\n",
    "            n_anc=2,\n",
    "        )\n",
    "        for group in [\"clump\", \"causal\"]:\n",
    "            snp_list = [snp for snp in dict_snp_list[group]]\n",
    "\n",
    "            dset_tmp = dset[snp_list]\n",
    "            cov_values = dset.indiv[[\"AVG_ANC\"]].values\n",
    "            cov_values = (cov_values - cov_values.mean(axis=0)) / cov_values.std(axis=0)\n",
    "            # heterogeneity test\n",
    "            df_tmp = admix_genet_cor.marginal_het(\n",
    "                geno=dset_tmp.geno,\n",
    "                lanc=dset_tmp.lanc,\n",
    "                y=df_pheno,\n",
    "                cov=cov_values,\n",
    "            )\n",
    "            # association test\n",
    "            df_tmp[\"assoc_p\"] = admix.assoc.marginal(\n",
    "                dset=dset_tmp,\n",
    "                pheno=df_pheno,\n",
    "                cov=cov_values,\n",
    "                method=\"ATT\",\n",
    "            ).P.values\n",
    "            df_tmp[\"snp\"] = dset_tmp.snp.index.values\n",
    "            dict_df_summary[group].append(df_tmp)\n",
    "\n",
    "    for group in [\"clump\", \"causal\"]:\n",
    "        df_tmp = pd.concat(dict_df_summary[group])\n",
    "        df_tmp = df_tmp[\n",
    "            [\n",
    "                \"snp\",\n",
    "                \"het_pval\",\n",
    "                \"coef1\",\n",
    "                \"se1\",\n",
    "                \"coef2\",\n",
    "                \"se2\",\n",
    "                \"assoc_p\",\n",
    "            ]\n",
    "        ]\n",
    "        df_tmp.to_csv(\n",
    "            f\"out/summary/{config}/sim_{sim_i}.{group}.tsv\",\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "\n",
    "\n",
    "def submit_job(config, n_sim=100):\n",
    "\n",
    "    if not os.path.exists(f\"out/clump/{config}\"):\n",
    "        os.makedirs(f\"out/clump/{config}\")\n",
    "\n",
    "    if not os.path.exists(f\"out/summary/{config}\"):\n",
    "        os.makedirs(f\"out/summary/{config}\")\n",
    "\n",
    "    for sim_i in range(n_sim):\n",
    "        if os.path.exists(f\"out/summary/{config}/sim_{sim_i}.causal.tsv\"):\n",
    "            continue\n",
    "        try:\n",
    "            # in case no clump was formed\n",
    "            gwas_clump(config, sim_i)\n",
    "            test_het(config, sim_i)\n",
    "        except ValueError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bfd58f-54f2-4f04-ad9f-0d0fb649a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the simulation parameters\n",
    "df_params = pd.DataFrame(\n",
    "    [\n",
    "        params\n",
    "        for params in itertools.product(\n",
    "            [f\"{i}_20_80\" for i in range(100)] + [f\"{i}_80_20\" for i in range(100)],\n",
    "            [0.004],\n",
    "            [1, 41],\n",
    "        )\n",
    "    ],\n",
    "    columns=[\"region\", \"hsq\", \"ncausal\"],\n",
    ")\n",
    "df_params[\"prefix\"] = df_params.apply(\n",
    "    lambda row: f\"region-{row.region}-hsq-{row.hsq}-ncausal-{int(row.ncausal)}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_todo_params = df_params[\n",
    "    ~df_params.apply(\n",
    "        lambda x: os.path.exists(join(\"out/summary\", x.prefix + f\"/sim_99.causal.tsv\")),\n",
    "        axis=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5237eb2-4250-4311-9bfa-7c061b77738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = submitit.SgeExecutor(folder=\"./submitit-logs\")\n",
    "\n",
    "executor.update_parameters(\n",
    "    time_min=180,\n",
    "    memory_g=26,\n",
    "    setup=[\n",
    "        \"export PATH=~/project-pasaniuc/software/miniconda3/bin:$PATH\",\n",
    "        \"export PYTHONNOUSERSITE=True\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "jobs = executor.map_array(submit_job, df_todo_params.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2c348-3a70-4e3b-a01b-510376c9686e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
